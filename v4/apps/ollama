{"captainVersion":4,"services":{"$$cap_appname":{"image":"ollama/ollama:$$cap_ollama_version","volumes":["$$cap_appname-data:/root/.ollama"],"restart":"always","caproverExtra":{"containerHttpPort":11434}}},"caproverOneClickApp":{"variables":[{"id":"$$cap_ollama_version","label":"Ollama Version","defaultValue":"0.1.17","description":"Check out their Docker page for the valid tags https://hub.docker.com/r/ollama/ollama/tags","validRegex":"/.{1,}/"}],"instructions":{"start":"Deploying Ollama for running local LLMs.","end":"Ollama has been deployed! Access the API at http://$$cap_appname.$$cap_root_domain. Run 'docker exec -it $(docker ps | grep ollama | awk '{print $1}') ollama run llama2' to download and run models."},"displayName":"Ollama","isOfficial":false,"description":"Run large language models locally. Get up and running with Llama 2, Code Llama, and other models on your own infrastructure."}}
